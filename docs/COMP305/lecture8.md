# Lecture 08
<font size="4">Hongye Qian</font> 

### Topic 3 Hebbâ€™s Rules
#### 1. MP neuron vs. Brain Function
-  storing information -> to store knowledge
-   producing logical and arithmetical operations on it -> to apply the knowledge stored to solve problems

#### 2. ANN Learning Rules
- Objective: We need a new model, of which the parameters are easily changeable (to be learnt).
![0223bd00d3e52e10dd10d7c3a19ab09.png](https://s2.loli.net/2024/10/25/rlQGnFdP72vDjZe.png)
- The ideal free parameters to adjust, and so to resolve learning, are the weights of connections ğ’˜. â€œç†æƒ³çš„å¯è°ƒè‡ªç”±å‚æ•°â€ æ˜¯è¿™äº›è¿æ¥çš„æƒé‡ï¼Œå³ ğ‘¤ï¼Œè°ƒæ•´è¿™äº›æƒé‡çš„å€¼å°±æ˜¯ç¥ç»ç½‘ç»œå­¦ä¹ çš„æ ¸å¿ƒå†…å®¹ã€‚
- From now, we do ***NOT*** have the restriction on the weight. That is, the weight can be any real value.
- We do ***NOT*** check the inhibitory input.
- the reason for the above two things:  The motivation is different! We do not consider the propositional logic here.Instead, we think about learning! ***Motivations are different. Thus, the models are different.***
- **ANN learning rule**: <u>***the rule how to adjust the weights of connections to get desirable output.***</u>
-  Much work in Artificial Neural Networks focuses on the learning rules that define how to connections change the weights of between neurons to better adapt a network to serve some overall function.
-  Background: ç¬¬ä¸€æ¬¡æå‡ºåœ¨1940sï¼Œ from psychological studies of **Donald Hebb** and **Frank Rosenblatt**.

### 3. Hebbâ€™s Rule (1949)
- Hebb proposed that a particular type of use-dependent modification of the connection strength of synapses might underlie  learning in the nervous system.
- æˆ‘æŸ¥çš„èµ„æ–™ï¼š
  1. **èµ«å¸ƒè§„åˆ™**æå‡ºäº†ä¸€ä¸ªå…³äºç¥ç»ç³»ç»Ÿå­¦ä¹ çš„å‡è®¾ï¼Œè®¤ä¸ºé€šè¿‡ä½¿ç”¨ä¾èµ–æ€§ä¿®æ”¹çªè§¦è¿æ¥çš„å¼ºåº¦ï¼Œå¯èƒ½æ˜¯ç¥ç»ç³»ç»Ÿä¸­å­¦ä¹ çš„åŸºç¡€ã€‚
  2. ç®€å•æ¥è¯´ï¼Œ**â€œç”¨è¿›åºŸé€€â€** çš„åŸåˆ™æ˜¯èµ«å¸ƒè§„åˆ™çš„æ ¸å¿ƒï¼Œå³çªè§¦è¿æ¥çš„å¼ºåº¦ä¼šéšç€ä½¿ç”¨é¢‘ç‡çš„å¢åŠ è€Œå¢å¼ºã€‚è¿™æ„å‘³ç€å¦‚æœä¸¤ä¸ªç¥ç»å…ƒç»å¸¸ä¸€èµ·æ¿€æ´»ï¼Œå®ƒä»¬ä¹‹é—´çš„è¿æ¥å°±ä¼šå˜å¾—æ›´å¼ºï¼Œä»è€Œå®ç°å­¦ä¹ ã€‚ 
- A neurophysiological postulate : When an axon of cell A  is near enough to excite a cell B and repeatedly and persistently takes part in firing it, some growth process or metabolic(æ–°é™ˆä»£è°¢çš„) change takes place in one or both cells, such that Aâ€™s efficiency as one of the cells firing B, is increased. 
- In another word: Cells that fire together, wire(n.é‡‘å±ä¸ v.æ¥é€šï¼Œè¿æ¥) together.
- History:  An experimental confirmation to Hebbâ€™s hypothesis came much later: in **1970s**, physiologists **Tim Bliss** and **Terje Lomo** discovered  ***the long-term potentiation, a sustained state of increased synaptic efficacy consequent to intense synaptic activity***.
- The conditions that Hebb predicted would lead to changes in synaptic strength have now been found to cause **the long-term potentiation** in some neurons of **hippocampus** and other brain areas. 
![b4aa614a2e7fa93e94778e4922cafdd.png](https://s2.loli.net/2024/10/26/QmtGvW3Ajqgc1Mo.png)
-  the weight $w_i^t$ is what we want to learn. æƒé‡$w_i^t$æ˜¯æˆ‘ä»¬è¦å­¦ä¹ çš„å†…å®¹ã€‚é€šè¿‡è°ƒæ•´æƒé‡ï¼Œç¥ç»å…ƒå¯ä»¥æ ¹æ®è¾“å…¥çš„å˜åŒ–æ¥ä¼˜åŒ–è¾“å‡ºã€‚
-  Again, here we allow $w_i^t$ could be other than -1 or 1, and we do not check inhibitory inputs. It is **NOT an MP neuron**. æƒé‡$w_i^t$ çš„å–å€¼å¯ä»¥ä¸æ˜¯ç®€å•çš„ -1 æˆ– 1ï¼Œè¿™ä¸æŸäº›ç‰¹å®šç¥ç»å…ƒæ¨¡å‹ä¸åŒï¼Œæ¯”å¦‚ MPç¥ç»å…ƒï¼ˆMcCulloch-Pittsç¥ç»å…ƒï¼‰ï¼Œåè€…åªæœ‰ -1 å’Œ 1 çš„ç¦»æ•£æƒé‡é€‰æ‹©ã€‚åœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œä¸è€ƒè™‘æŠ‘åˆ¶æ€§è¾“å…¥ï¼Œå› æ­¤ä¸æ˜¯ MP ç¥ç»å…ƒã€‚
-  The simplest formulation of the rule:  Increase weight 
of connection at every next instant in the way. èµ«å¸ƒè§„åˆ™çš„åŸºæœ¬æ€æƒ³æ˜¯ï¼Œæ¯å½“ç¥ç»å…ƒåŒæ—¶æ¿€æ´»æ—¶ï¼Œè¿æ¥æƒé‡å°±ä¼šå¢åŠ ã€‚
- $w_i^{t+1} = w_i^t + \Delta w_i^t$ where $\Delta w_i^t = C a_i^t X^{t+1}$ 
- 1. $w_i^{t+1} = w_i^t + \Delta w_i^t$ è¡¨ç¤ºæƒé‡$w_i$ åœ¨ $t+1$ æ—¶åˆ»çš„å€¼ç­‰äºä¸Šä¸€æ—¶åˆ» ğ‘¡ çš„å€¼åŠ ä¸Šä¸€ä¸ªå˜åŒ–é‡ $\Delta w_i^t$
  2. $\Delta w_i^t = C a_i^t X^{t+1}$ è¡¨ç¤ºæƒé‡çš„å˜åŒ–é‡ï¼Œå–å†³äºè¾“å…¥ $a_i$ ,è¾“å‡º$X^{t+1}$, learning rate $C$
- 1. $w_i^t$  is the weight of connection at instant $t$
  2. $w_i^{t+1}$ is the weight of connection at the next instant $t+1$
  3. $\Delta w_i^t$  is the increment by which the weight of connection is enlarged
  4. $C$  is positive coefficient which determines learning rate.
  5. $a_i^t$  is input value from the presynaptic neuron at instant $t$
  6. $X^{t+1}$  is outputof the postsynaptic neuron at the instant $t+1$.
- Thus, the weight of connection changes at the next instant only if both preceding input via this connection and the resulting output simultaneously are not equal to 0.
-  1. Input is not equal to 0. -> Excitatory input
   2.  Output is not equal to 0. -> Neuron is fired.
-  The second equation emphasizes the synapse correlation nature of a Hebbian. äººè¯ï¼šç¬¬äºŒä¸ªå…¬å¼ï¼ˆä¹Ÿå°±æ˜¯ä¹‹å‰å±•ç¤ºçš„æƒé‡æ›´æ–°å…¬å¼ï¼‰å¼ºè°ƒäº†**Hebbiançªè§¦çš„ç›¸å…³æ€§æ€§è´¨**ï¼Œå³çªè§¦çš„å¼ºåº¦å˜åŒ–æ˜¯åŸºäºè¾“å…¥å’Œè¾“å‡ºçš„ç›¸å…³æ€§ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœä¸¤ä¸ªç¥ç»å…ƒç»å¸¸ä¸€èµ·æ¿€æ´»ï¼Œé‚£ä¹ˆå®ƒä»¬ä¹‹é—´çš„è¿æ¥å°±ä¼šåŠ å¼ºã€‚
- Sometimes the Hebbâ€™s rule is referred to as activity product rule.
- Hebbâ€™s original learning rule referred exclusively to excitatory synapsesã€‚

### 4. Algorithm of Hebbâ€™s Rule for a Single Neuron (NOT MP)
![bd37c7bc68406b599f9dacc4e3795d1.png](https://s2.loli.net/2024/10/26/HR9NXjw2VzEmLhF.png)





 










