# Lecture 12
<font size="4">Hongye Qian</font> 

### Topic 5 Kohonenâ€™s Rule (Competitive Learning)
#### 1. Kohonen Rule: Competitive Learning
- We consider a one-layer neural network with multiple outputs.
- In competitive learning, as the name implies, an output neuron of a network compete among all the outputs to be updated (regarding the weights). åœ¨ç«äº‰å­¦ä¹ ä¸­ï¼Œè¾“å‡ºå±‚çš„ç¥ç»å…ƒä¹‹é—´ç›¸äº’ç«äº‰ï¼Œä»¥å†³å®šå“ªä¸ªç¥ç»å…ƒçš„æƒé‡éœ€è¦æ›´æ–°ã€‚å³ï¼Œè¾“å‡ºç¥ç»å…ƒä¼šç«äº‰ï¼Œåªæœ‰ä¸€ä¸ªè·èƒœçš„ç¥ç»å…ƒä¼šè¢«æ›´æ–°
- Whereas in a neural network based on Hebbian learning several output neurons may be updated simultaneously, in competitive learning **only a single output neuron is updated at any instant**.  ä¸åŒäºHebbianå­¦ä¹ æ–¹æ³•ï¼ˆå¯ä»¥åŒæ—¶æ›´æ–°å¤šä¸ªè¾“å‡ºç¥ç»å…ƒçš„æƒé‡ï¼‰ï¼Œç«äº‰å­¦ä¹ ä¸­ä¸€æ¬¡åªæ›´æ–°ä¸€ä¸ªè·èƒœçš„è¾“å‡ºç¥ç»å…ƒã€‚
- This makes competitive learning highly suited to discover statistically important features that may be used to classify a set of input patterns. ç«äº‰å­¦ä¹ èƒ½å¤Ÿå¸®åŠ©ç¥ç»ç½‘ç»œå‘ç°é‡è¦çš„ç»Ÿè®¡ç‰¹å¾ï¼Œç”¨äºå¯¹è¾“å…¥æ¨¡å¼è¿›è¡Œåˆ†ç±»ã€‚
- Teuvo Kohonen suggested a network in which the output neurons compete for the right to respond to an input. Teuvo Kohonen suggested a network in which the output neurons compete for the right to respond to an input. Teuvo Kohonenæå‡ºäº†ä¸€ç§ç½‘ç»œç»“æ„ï¼Œåœ¨è¿™ç§ç½‘ç»œä¸­ï¼Œè¾“å‡ºç¥ç»å…ƒä¹‹é—´ä¼šç«äº‰ï¼Œä»¥è·å¾—å¯¹è¾“å…¥çš„å“åº”æƒåˆ©ã€‚
- arning rule first assumes that one of the output neurons, say the ğ‘—-th, has maximum value of the instant state $S_j$ at that instant.  å­¦ä¹ è§„åˆ™å‡è®¾æŸä¸ªè¾“å‡ºç¥ç»å…ƒï¼ˆæ¯”å¦‚ç¬¬ ğ‘— ä¸ªç¥ç»å…ƒï¼‰åœ¨æŸä¸€æ—¶åˆ»å…·æœ‰æœ€å¤§å€¼$S_j$ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™ä¸ªç¥ç»å…ƒçš„æ¿€æ´»çŠ¶æ€æœ€é«˜ã€‚
-  neuron is declared to be the winner, and only the weights of the winnerâ€™s connections $w_j = (w_{j1}, w_{j2}, \cdots, w_{jn})$   è¿™ä¸ªå…·æœ‰æœ€å¤§æ¿€æ´»çŠ¶æ€çš„ç¥ç»å…ƒè¢«ç§°ä¸ºâ€œèƒœè€…â€ï¼Œåªæœ‰èƒœè€…ç¥ç»å…ƒçš„æƒé‡$w_j = (w_{j1}, w_{j2}, \cdots, w_{jn})$ä¼šå¾—åˆ°æ›´æ–°ã€‚
![b33bf6d3dcfac8ca28a7b4cf9a0947e.png](https://s2.loli.net/2024/10/29/zgcvQ8Gy5CalFM9.png)
- We further relax the restriction on the input type, i.e., we allow real inputs in the competitive learning. åœ¨è¿™ä¸ªç«äº‰å­¦ä¹ æ¨¡å‹ä¸­ï¼Œè¾“å…¥ç±»å‹çš„é™åˆ¶è¢«æ”¾å®½äº†ï¼Œå…è®¸è¾“å…¥ä¸ºå®æ•°ï¼ˆreal inputsï¼‰ã€‚è¿™æ„å‘³ç€è¾“å…¥æ•°æ®å¯ä»¥æ˜¯è¿ç»­çš„æ•°å€¼ï¼Œè€Œä¸ä»…é™äºç¦»æ•£çš„æ•°å€¼ã€‚
- Only the winner outputs a 1, while others output 0. åœ¨ç«äº‰å­¦ä¹ ä¸­ï¼Œåªæœ‰èƒœå‡ºçš„ç¥ç»å…ƒï¼ˆèƒœè€…ï¼‰ä¼šè¾“å‡º1ï¼Œå…¶ä»–ç¥ç»å…ƒåˆ™è¾“å‡º0ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåªæœ‰ä¸€ä¸ªç¥ç»å…ƒä¼šæ¿€æ´»è¾“å‡º1ï¼Œå…¶ä»–ç¥ç»å…ƒåœ¨å½“å‰è¾“å…¥ä¸‹éƒ½è¢«æŠ‘åˆ¶ï¼Œè¾“å‡ºä¸º0ã€‚
- The learning rule first assumes that one of the output neurons, say the $j$-th, has maximum value state ğ‘† at that instant. å­¦ä¹ è§„åˆ™å‡è®¾åœ¨æŸä¸€æ—¶åˆ»ï¼Œè¾“å‡ºç¥ç»å…ƒä¸­æŸä¸€ä¸ªï¼ˆä¾‹å¦‚ç¬¬ ğ‘— ä¸ªç¥ç»å…ƒï¼‰çŠ¶æ€å€¼æœ€å¤§ï¼Œå³ $S_j = \max(S_1, \cdots, S_m)$
- That neuron is declared the winner, and only its vector of connections weights. $w_j = (w_{j1}, w_{j2}, \cdots, w_{jn})$è¯¥çŠ¶æ€å€¼æœ€å¤§çš„ç¥ç»å…ƒè¢«ç§°ä¸ºâ€œèƒœè€…â€ã€‚åœ¨ç«äº‰å­¦ä¹ ä¸­ï¼Œåªæœ‰è¿™ä¸ªâ€œèƒœè€…â€ç¥ç»å…ƒçš„è¿æ¥æƒé‡å‘é‡ $w_j = (w_{j1}, w_{j2}, \cdots, w_{jn})$ ä¼šè¢«æ›´æ–°ï¼Œå…¶ä»–ç¥ç»å…ƒçš„æƒé‡ä¸ä¼šå˜åŒ–ã€‚
- è¿™ç§æ–¹å¼ç¡®ä¿äº†æ¯æ¬¡è®­ç»ƒä¸­åªæœ‰ä¸€ä¸ªè¾“å‡ºç¥ç»å…ƒçš„æƒé‡ä¼šæ›´æ–°ï¼Œä»è€Œé€æ¸å¢å¼ºè¯¥ç¥ç»å…ƒå¯¹ç‰¹å®šè¾“å…¥æ¨¡å¼çš„å“åº”èƒ½åŠ›ã€‚
- Assume that the ğ‘—-th output neuron has maximum weighted input at that instant ğ‘¡. 
$S_j^t = \max(S_1^t, \cdots, S_m^t)$ å‡è®¾åœ¨æŸä¸€æ—¶åˆ» ğ‘¡ï¼Œç¬¬ ğ‘— ä¸ªè¾“å‡ºç¥ç»å…ƒå…·æœ‰æœ€å¤§çš„åŠ æƒè¾“å…¥ä¸ºå‰é¢è¿™ä¸ªå¼å­
- Then its weight is updated as $w_{ji}^{t+1} = w_{ji}^t + \Delta w_{ji}^t$ where ğ‘– = 1,â‹¯,ğ‘›.
- The incremental term $\Delta w_{ji}^t = C(a_i^t - w_{ji}^t)$
- Having the maximum weighted input, means that the winning output neuron has the vector of connection weights most similar to the input vector.
- The winner-takes-it-all learning rule modifies the winnerâ€™s (only) connection weights by a fraction (learning rate) of the difference between the current input vector and the current weight vector of the winner neuron. â€œèƒœè€…å…¨å¾—â€ï¼ˆWinner-takes-it-allï¼‰å­¦ä¹ è§„åˆ™æ˜¯ä¸€ç§ç”¨äºç¥ç»ç½‘ç»œçš„æƒé‡æ›´æ–°æ–¹æ³•ã€‚åœ¨è¿™ç§è§„åˆ™ä¸‹ï¼Œåªæœ‰å½“å‰è¾“å…¥ä¸‹çš„â€œèƒœè€…â€ç¥ç»å…ƒçš„è¿æ¥æƒé‡ä¼šè¢«æ›´æ–°ï¼Œå…¶ä»–ç¥ç»å…ƒçš„æƒé‡ä¸å˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™ä¸ªæ›´æ–°è¿‡ç¨‹æ˜¯é€šè¿‡è°ƒæ•´â€œèƒœè€…â€ç¥ç»å…ƒçš„æƒé‡å‘é‡ï¼Œä½¿å®ƒæ›´æ¥è¿‘è¾“å…¥å‘é‡ã€‚
- Q & A About Kohonen Learning Rule: Please see the lecture12 P21. The writer's battery is low. 
![1682513697fc84471b6aa1dfa48f36d.png](https://s2.loli.net/2024/10/29/9nsdqjtTU3LYIkH.png)
- Example:
![57df276bcda322db94c0017f1b816b7.png](https://s2.loli.net/2024/10/29/paEytQDHjJLlc7A.png)
- Round 1: 
  Consider the input (2,1). Learning rate is 0.5.
  $S_1 = 4 \times 2 + 1 \times 1 = 9$
  $S_2 = 2 \times 2 + 4 \times 1 = 8$
  thus we update the weights of the 1st output.
  $w_{11} = w_{11} + \Delta w_{11} = 4 + 0.5 \times (2 - 4) = 3$
  $w_{21} = w_{21} + \Delta w_{21} = 1 + 0.5 \times (1 - 1) = 1$
![63eff53984347c92bd2c0270ad4c4a1.png](https://s2.loli.net/2024/10/29/gtpH4q9iCFGuEJR.png)
- Round 2:
  Consider the input (2,1). Learning rate is 0.5.
  $S_1 = 3 \times 2 + 1 \times 1 = 7$
  $S_2 = 2 \times 2 + 4 \times 1 = 8$
  thus we update the weights of the 2nd output.
  $w_{12} = w_{12} + \Delta w_{12} = 2 + 0.5 \times (2 - 2) = 2$
  $w_{22} = w_{22} + \Delta w_{22} = 4 + 0.5 \times (1 - 4) = 2.5$
![e970c58479cdd98cf2d63707206c7a6.png](https://s2.loli.net/2024/10/29/76jQhWzkde2aFK5.png)
- Round 3:
  Consider the input (2,1). Learning rate is 0.5.
  $S_1 = 3 \times 2 + 1 \times 1 = 7$
  $S_2 = 2 \times 2 + 4 \times 1 = 8$
   thus we update the weights of the 2nd output
  $w_{12} = w_{12} + \Delta w_{12} = 2 + 0.5 \times (2 - 2) = 2$
  $w_{22} = w_{22} + \Delta w_{22} = 4 + 0.5 \times (1 - 4) = 2.5$
  ![e970c58479cdd98cf2d63707206c7a6.png](https://s2.loli.net/2024/10/29/76jQhWzkde2aFK5.png)

#### 2. Self-Organizing Map (SOM)
![5cc2f433d21b61a6f13616ba96c8709.png](https://s2.loli.net/2024/10/29/roGLh7HyNVD3dnb.png)
- We are interested in a specific application of Kohonen learning rule (competitive learning), self-organizing map (SOM).  SOMæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œåˆ©ç”¨ç«äº‰å­¦ä¹ è§„åˆ™æ¥å®ç°é«˜ç»´æ•°æ®çš„é™ç»´ï¼ŒåŒæ—¶ä¿æŒæ•°æ®çš„æ‹“æ‰‘ç»“æ„ã€‚å®ƒæ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå¸¸ç”¨äºæ•°æ®çš„èšç±»å’Œå¯è§†åŒ–ã€‚
- SOM is used to produce a lowdimensional (typically two-dimensional) representation of a higher dimensional data set while preserving the topological structure of the data. For instance, in left 
figure, a SOM maps a ğ‘›-dimensional input to a 2-dimensional space. It can be used for clustering or visualization. SOMå¯ä»¥å°†é«˜ç»´æ•°æ®ï¼ˆä¾‹å¦‚ ğ‘› ç»´è¾“å…¥ï¼‰æ˜ å°„åˆ°ä½ç»´ï¼ˆé€šå¸¸æ˜¯äºŒç»´ï¼‰ç©ºé—´ã€‚è¿™æ ·å¯ä»¥åœ¨ä¿æŒæ•°æ®çš„æ‹“æ‰‘ç»“æ„çš„åŒæ—¶ï¼Œä»¥å›¾å½¢åŒ–çš„æ–¹å¼å±•ç¤ºå¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚
- The training of such specific networks is based on Kohonen learning rule (competitive learning). è®­ç»ƒå°±ç”¨ä¸Šé¢é‚£ä¸ªruleçš„å…¬å¼ã€‚
![2e2eb9efaae58680885e7248b3fa7be.png](https://s2.loli.net/2024/10/29/S1Y4nb3Foz9N5Uv.png)
- Sometimes, the winning neighbourhood is extended beyond the single neuron winner, so that it includes the neighbouring neurons, for which some corrections to the connection weights may also be done. æœ‰æ—¶ï¼Œæ›´æ–°èŒƒå›´ä¼šæ‰©å¤§åˆ°èƒœè€…ç¥ç»å…ƒçš„é‚»è¿‘ç¥ç»å…ƒï¼Œè€Œä¸ä»…ä»…æ˜¯èƒœè€…è‡ªèº«ã€‚è¿™ç§æ‰©å±•æœ‰åŠ©äºè°ƒæ•´è¿æ¥æƒé‡ï¼Œä»è€Œåœ¨ç½‘ç»œä¸­æ›´å¥½åœ°æ•æ‰æ•°æ®çš„æ‹“æ‰‘ç»“æ„ã€‚
- Inaddition, ğ¶ may sometimes decrease along with the time, for convergence purpose, i.e., ğ¶(ğ‘¡) âŸ¶0, ğ‘¡âŸ¶0  ä¸ºäº†ç¡®ä¿ç®—æ³•çš„æ”¶æ•›ï¼Œå³é€æ­¥æ¥è¿‘ç¨³å®šçŠ¶æ€ï¼Œå­¦ä¹ ç‡ ğ¶ æœ‰æ—¶ä¼šéšç€æ—¶é—´ ğ‘¡ çš„å¢åŠ è€Œé€æ¸å‡å°ã€‚å…·ä½“è€Œè¨€ï¼Œå½“ ğ‘¡ â†’0 æ—¶é—´è¶‹å‘æ— ç©·ï¼‰æ—¶ï¼Œå­¦ä¹ ç‡ ğ¶(ğ‘¡) ä¼šè¶‹è¿‘äº 0ã€‚
- In the map, location of the most strongly excited 
neurons (winner) is correlated with the certain input signals. 
- Neighbouring excited neurons correspond to inputs with similar features.
- Because in the training phase weights of the whole neighbourhood are moved in the same direction, similar items tend to excite adjacent neurons. Therefore, SOM forms a semantic map where similar samples are mapped close together and dissimilar ones apart. 
- Example å‚è€ƒ lecture11 P33


















