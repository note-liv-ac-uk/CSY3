# Lecture 18
<font size="4">Hongye Qian</font> 

## Topic 5 Multi-Layer Perceptron
### Gradient Decent and Activation Function Design
### Error Function for a Single Input
![5cbe1ddf049430aa8aea470ef1ea69a.png](https://s2.loli.net/2024/11/28/Xao7lLfQGqz9Mvx.png)
The output error of a multilayer perceptron for the $k$-th input pattern is **a half of the squared error**:  $E^k$ è¡¨ç¤ºå¤šå±‚æ„ŸçŸ¥å™¨å¯¹ç¬¬ $k$ ä¸ªè¾“å…¥æ ·æœ¬çš„è¾“å‡ºè¯¯å·®ã€‚è¯¯å·®é‡‡ç”¨**å¹³æ–¹è¯¯å·®çš„ä¸€åŠ**å½¢å¼ã€‚

$$
E^k = \frac{1}{2} \sum_{j=1}^m \left(e_j^k\right)^2 = \frac{1}{2} \sum_{j=1}^m \left(t_j^k - X_j^k\right)^2
$$

The performance of multilayer perceptron= $\frac{1}{2}$the total squared error
$$
E = \sum_{k=1}^r E^k = \frac{1}{2} \sum_{k=1}^r \sum_{j=1}^m \left(e_j^k\right)^2 = \frac{1}{2} \sum_{k=1}^r \sum_{j=1}^m \left(t_j^k - X_j^k\right)^2
$$
- During the training stage, the input value $a$ to the network are the input vectors of the training set, which are **constant parameters** for the process.

- Therefore, the **MLP error function $E$** is a function of the weights of connections only:

$$
E = \sum_{k=1}^r E^k = \frac{1}{2} \sum_{k=1}^r \sum_{j=1}^m \left(e_j^k\right)^2 = \frac{1}{2} \sum_{k=1}^r \sum_{j=1}^m \left(t_j^k - F_j(w^l, w^{l-1}, \dots, w^1, a^k)\right)^2 = E(W)
$$
- å› ä¸ºerrorä»…ä»…å—è¾“å…¥å’Œæƒé‡å½±å“ï¼Œè¾“å…¥è¢«è§†ä½œæ˜¯å¸¸æ•°ï¼Œæ‰€ä»¥å®ƒå¯ä»¥è¢«è§†ä½œä¸ºæƒé‡çš„ä¸€ä¸ªå‡½æ•°ã€‚
- <u>The better the MLP performs, the smaller the MLP error function $E$is.</u>
- Thus MLP learning can be considered as the optimization problem: $
\min_W E(W)
$, å¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™æ¥å®ç°ï¼ˆæƒ³å¿…å¤§å®¶éƒ½å¾ˆç†Ÿæ‚‰è¿™ä¸ªä¸œè¥¿ï¼‰ï¼Œæ¯”å¦‚SGD
### Gradient Decent
![0e4a61cf3ee82306b2e5ec4ce9ae4ed.png](https://s2.loli.net/2024/11/29/FXcJePuO4wxY3lv.png)
Gradient descent is based on the observation that if the multi-variable function $F(x)$ is defined and differentiable in a neighborhood of a point $a$, then $F(x)$ decreases fastest if one goes from $a$ in the direction of the negative gradient of $F$ at $a$, $-\nabla F(a)$. It follows that, if

$$
a' = a + \gamma(-\nabla F(a)) = a - \gamma \nabla F(a)
$$

For a $\gamma \in \mathbb{R}^+$ small enough, then $F(a) \geq F(a')$
***
è§£é‡Šï¼š

- æ¢¯åº¦ä¸‹é™çš„ä¾æ®ï¼š
- å¦‚æœä¸€ä¸ªå¤šå˜é‡å‡½æ•° $F(x)$ åœ¨æŸç‚¹ $a$ çš„é‚»åŸŸå†…å®šä¹‰å¹¶ä¸”å¯å¾®ï¼Œé‚£ä¹ˆå‡½æ•° $F(x)$ åœ¨ $a$ ç‚¹çš„å˜åŒ–æœ€å¿«çš„æ–¹å‘æ˜¯è´Ÿæ¢¯åº¦æ–¹å‘ $-\nabla F(a)$ã€‚

- æ›´æ–°å…¬å¼ï¼š
æ¢¯åº¦ä¸‹é™æ³•é€šè¿‡ä»¥ä¸‹å…¬å¼æ›´æ–°å˜é‡ï¼š
$$
a' = a + \gamma(-\nabla F(a)) = a - \gamma \nabla F(a)
$$
å…¶ä¸­ï¼š
- $a$ï¼šå½“å‰ç‚¹ã€‚
- $\nabla F(a)$ï¼šå‡½æ•° $F(x)$ åœ¨ $a$ ç‚¹çš„æ¢¯åº¦ã€‚
- $\gamma$ï¼šå­¦ä¹ ç‡ï¼ˆä¸€ä¸ªæ­£çš„å°å€¼ï¼Œæ§åˆ¶æ›´æ–°æ­¥é•¿å¤§å°ï¼‰ã€‚

    æ¡ä»¶ï¼š
- å½“ $\gamma$ è¶³å¤Ÿå°æ—¶ï¼Œæ–°ç‚¹ $a'$ çš„å‡½æ•°å€¼æ˜¯å°äºæˆ–ç­‰äºå½“å‰ç‚¹çš„å‡½æ•°å€¼ï¼š
$$
F(a) \geq F(a')
$$

è¿™è¡¨ç¤ºæ¯æ¬¡æ›´æ–°åï¼Œå‡½æ•°å€¼éƒ½æœç€æœ€å°å€¼æ–¹å‘å˜åŒ–ã€‚
***
å¯¹åº”çš„ç­‰é«˜çº¿å›¾ï¼š
![578777d4cee8aaaf3994a32f0fe3a68.png](https://s2.loli.net/2024/11/29/bMUT7CdEv3lDqgk.png)
#### Gradient Descent

Gradient descent is based on the observation that if the multi-variable function $F(x)$ is defined and differentiable in a neighborhood of a point $a$, then $F(x)$ decreases fastest if one goes from $a$ in the direction of the negative gradient of $F$ at $a$, $-\nabla F(a)$. It follows that, if

$$
a_{n+1} = a_n - \gamma \nabla F(a_n)
$$

For a $\gamma \in \mathbb{R}^+$ small enough, then $F(a_n) \geq F(a_{n+1})$.

---

With this observation, one starts with an initial guess $x_0$ for a local minimum of $F$, and considers the sequence $x_0, x_1, x_2, \cdots$ such that

$$
x_{n+1} = x_n - \gamma \nabla F(x_n).
$$

We can have monotonic sequence:
$$
F(x_0) \geq F(x_1) \geq F(x_2) \geq \cdots
$$

So hopefully, the sequence $\{x_n\}$ converges to the desired local minimum $x^*$, where:
$$
\nabla F(x^*) = 0
$$
- è§£é‡Šï¼š$$
x_{n+1} = x_n - \gamma \nabla F(x_n)
$$

- **$x_n$**ï¼šå½“å‰ç‚¹ã€‚
- **$\nabla F(x_n)$**ï¼šå½“å‰ç‚¹çš„æ¢¯åº¦ï¼ˆå¯¼æ•°ï¼‰ã€‚
- **$\gamma$**ï¼šå­¦ä¹ ç‡ï¼Œæ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•¿ã€‚

---

å•è°ƒæ€§
å¦‚æœå­¦ä¹ ç‡ $\gamma$ é€‰æ‹©å¾—å½“ï¼Œæ¯æ¬¡æ›´æ–°åç›®æ ‡å‡½æ•°çš„å€¼å•è°ƒå‡å°‘ï¼š
$$F(x_0) \geq F(x_1) \geq F(x_2) \geq \cdots$$

---

æ”¶æ•›æ€§
ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿­ä»£ç‚¹åºåˆ— $\{x_n\}$ å°†ä¼šæ”¶æ•›åˆ°ç›®æ ‡å‡½æ•°çš„å±€éƒ¨æœ€å°å€¼ $x^*$ï¼Œæ»¡è¶³ï¼š
$$\nabla F(x^*) = 0$$
***
### Learning of a Multilayer Perceptron
- Gradient descent method addresses the issue of how to update weights.
- One of the most popular techniques is called **error backpropagation**(åå‘ä¼ æ’­),  
where the error of output neurons is propagated back to derive the weight adjustment of a given hidden neuron, based on how much the neuron contributes to the output error. æ ¹æ®éšè—ç¥ç»å…ƒå¯¹è¾“å‡ºè¯¯å·®çš„è´¡çŒ®ç¨‹åº¦ï¼Œå°†è¾“å‡ºç¥ç»å…ƒçš„è¯¯å·®åå‘ä¼ æ’­ï¼Œä»¥è®¡ç®—éšè—ç¥ç»å…ƒçš„æƒé‡è°ƒæ•´ã€‚


- The **backpropagation** algorithm updates the weights of connections $w$ **computationally efficiently** based on the method of **gradient descent**. åå‘ä¼ æ’­æ˜¯æ·±åº¦å­¦ä¹ ä¸­è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ï¼Œç»“åˆäº†æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆä¼˜åŒ–ç½‘ç»œæƒé‡ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
- Gradient descent method addresses the issue of how to update weights.
- The backpropagation algorithm makes the weight updating efficient.
- The backpropagation ğ‘¤,ğ‘¤ ,â‹¯,ğ‘¤ ,ğ‘ algorithm looks for the minimum of the error function ğ¸ in the space of weights of connections ğ‘¤ using the method of gradient descent.
- The gradient of the multi-variable function $E$ is defined as:

$$ \nabla E = \left( \frac{\partial E}{\partial w_{11}^1}, \cdots, \frac{\partial E}{\partial w_{n_1 n_0}^1}, \frac{\partial E}{\partial w_{11}^2}, \cdots, \frac{\partial E}{\partial w_{n_2 n_1}^2}, \cdots, \frac{\partial E}{\partial w_{11}^l}, \cdots, \frac{\partial E}{\partial w_{n_l n_{l-1}}^l} \right) $$

$\frac{\partial E}{\partial w_{ji}^h}$ è¡¨ç¤ºè¯¯å·®å‡½æ•° $E$ å¯¹æƒé‡ $w_{ji}^h$ çš„åå¯¼æ•°ã€‚è¿™æ˜¯ $E$ ç›¸å¯¹äºç¬¬ $h$ å±‚ä¸­ç¬¬ $j$ ä¸ªç¥ç»å…ƒå’Œå‰ä¸€å±‚ï¼ˆç¬¬ $h-1$ å±‚ï¼‰ä¸­ç¬¬ $i$ ä¸ªç¥ç»å…ƒä¹‹é—´è¿æ¥æƒé‡çš„åå¯¼æ•°ã€‚
-  Vector âˆ‡ğ¸ is called gradient of the error function ğ¸, and it points in the direction along which ğ¸ increases most rapidly.
-   **We would like to go in the opposite direction to most rapidly minimize ğ¸.**
- Therefore, during the **iterative process of gradient descent**, each weight of connection, including the hidden ones, is updated:

$$w_{ji}^h = w_{ji}^h + \Delta w_{ji}^h, \quad \text{where } \Delta w_{ji}^h = -C \frac{\partial E}{\partial w_{ji}^h}$$

Here $C$ represents the learning rate as before. 
-  Since calculus-based methods of minimization depends on the taking of derivatives, their application to network training requires the error function ğ‘¬ be a differentiable function (almost everywhere). å‡½æ•°è‡³å°‘åœ¨ç»å¤§å¤šæ•°åœ°æ–¹è¦æ±‚æ˜¯å¯å¾®çš„
### Generic sigmoidal activation function
The sigmoidal function is defined as:

$$f(S) = \frac{\alpha}{1 + e^{-\beta S + \gamma}} + \lambda$$
- It has four parameters: $\alpha$, $\beta$, $\gamma$, $\lambda$.
- It is monotonically increasing. å®ƒæ˜¯**å•è°ƒé€’å¢**çš„ã€‚
- It has the shape of the s-curve, which is commonly used for learning processes. å…¶å½¢çŠ¶ä¸º S æ›²çº¿ï¼Œå¸¸ç”¨äºå­¦ä¹ è¿‡ç¨‹çš„å»ºæ¨¡ã€‚
![0d4536d7c66595f6768b688763e7d8d.png](https://s2.loli.net/2024/11/29/gZjBpsXMoOhYn14.png)
- The parameter $\beta$ has the most significant effect on the slope of this curve:
  - A small value of $\beta$ corresponds to a gradual curve increase, while its large value corresponds to a steep increase. è¾ƒå°çš„ $\beta$ å€¼å¯¹åº”æ›²çº¿çš„é€æ¸å¢é•¿ã€‚è¾ƒå¤§çš„ $\beta$ å€¼å¯¹åº”æ›²çº¿çš„é™¡å³­å¢é•¿ã€‚
  - The case $\beta = \infty$ corresponds to a hard-limiting step function. å½“ $\beta = \infty$ æ—¶ï¼Œæ›²çº¿å˜ä¸ºç¡¬é™åˆ¶é˜¶è·ƒå‡½æ•°ï¼ˆhard-limiting step functionï¼‰ã€‚
  - The product $\alpha\beta$ defines the **steepness** of the curve. å‚æ•° $\alpha\beta$ çš„ä¹˜ç§¯å†³å®šäº†æ›²çº¿çš„**é™¡å³­åº¦**ã€‚

- The product $\alpha\beta$ defines the **steepness** of the curve.
- The parameter $\gamma$ causes a shifting along the horizontal axis and is usually equal to zero. å‚æ•° $\gamma$ æ§åˆ¶æ›²çº¿åœ¨æ¨ªè½´ä¸Šçš„å¹³ç§»ï¼Œé€šå¸¸ä¸ºé›¶ã€‚
- The parameters $\alpha$ and $\lambda$ define the range limits for scaling purposes. å‚æ•° $\alpha$ å’Œ $\lambda$ ç”¨äºå®šä¹‰ç¼©æ”¾èŒƒå›´çš„ä¸Šä¸‹é™ã€‚
- See some examples on lec.18 P38
### Learning of a Multilayer Perceptron
*Generic sigmoidal activation function:*
$$f(S) = \frac{\alpha}{1 + e^{-\beta S + \gamma}} + \lambda$$

*Its derivative:*
$$f'(S) = \frac{df}{dS} = \frac{\beta}{\alpha} \cdot (f(S) + \lambda)(\alpha + \lambda - f(S))$$

*Explanation:*
It is straightforward to compute the derivative at any particular value of variable $S$ without actual differentiation,  
if the value of the activation function itself is known for that value of $S$. å¦‚æœå·²çŸ¥æ¿€æ´»å‡½æ•°åœ¨æŸç‰¹å®š $S$ å€¼å¤„çš„å€¼ï¼Œå¯ä»¥ç›´æ¥è®¡ç®—è¯¥ç‚¹çš„å¯¼æ•°ï¼Œè€Œæ— éœ€å®é™…è¿›è¡Œå¾®åˆ†æ“ä½œã€‚
- This feature of the sigmoid function is used for back propagation of corrections to weights of hidden neurons during multiple layer perceptron training process. 
- If all activation functions $f(S)$ in the network are differentiable, then we can use the **chain rule** to calculate the partial derivative of the error function $E$ with respect to the weight of a specific connection.














