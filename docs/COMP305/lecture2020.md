# Lecture 20
<font size="4">Hongye Qian</font> 

### Backpropagation
![8a9bf744ea25cd03480b2bd4748fcf3.png](https://s2.loli.net/2024/12/01/QnM6XVCwptIcGT9.png)
- The closer the connection is to the output, the simpler the formula of the partial derivative of its weight is. 
-  There are some overlap between two formulas, indicating it is possible to store the overlap to avoid duplicate computation.

### Formula simplification
æˆ‘æ„Ÿè§‰pptä¸Šçš„å…¬å¼æ¨å¯¼å°±æ˜¯ä¸¤çœ¼ä¸€é»‘ï¼Œå°±æ˜¯æ¨ã€‚ä»¥ä¸‹æ˜¯æˆ‘å°è¯•è§£é‡Šæ¸…æ¥šçš„æ¨å¯¼ã€‚
- åŸå…¬å¼ï¼š
$$\frac{\partial E}{\partial w_{j_0 i_0}^l} = \begin{cases} \left(X_{j_0}^{l_0} - t_{j_0}\right) \cdot \left(f_{j_0}^{l_0}\right)'(S_{j_0}^{l_0}) \cdot X_{i_0}^{l_0-1}, & \text{When } l = l_0 \\ \sum_{j=1}^{n^l} \left(X_j^l - t_j\right) \cdot \left(f_j^l\right)'(S_j^l) \cdot \sum_{i=1}^{n^{l-1}} w_{ji}^{l-1} \cdot \left(... \cdot \left(f_{j_0}^{l_0}\right)'(S_{j_0}^{l_0}) \cdot X_{i_0}^{l_0-1}\right), & \text{When } l \neq l_0 \end{cases}$$

-å’±ä»¬å…ˆçœ‹è¿™ä¸ªå…¬å¼ï¼š
$$\left(X_{j_0}^{l_0} - t_{j_0}\right) \cdot \left(f_{j_0}^{l_0}\right)'(S_{j_0}^{l_0}) \cdot X_{i_0}^{l_0-1}$$
- $\left(X_{j_0}^{l_0} - t_{j_0}\right)$è¿™ä¸ªè¡¨ç¤ºçš„æ˜¯è¿™ä¸ªèŠ‚ç‚¹çš„è¯¯å·®
- $\left(f_{j_0}^{l_0}\right)'(S_{j_0}^{l_0})$è¿™ä¸ªè¡¨ç¤ºçš„æ˜¯è¿™ä¸ªèŠ‚ç‚¹çš„æ¿€æ´»å‡½æ•°çš„å¯¼æ•°
- $X_{i_0}^{l_0-1}$è¿™ä¸ªè¡¨ç¤ºçš„æ˜¯å‰ä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºå€¼
- è¿™ä¸€éƒ¨åˆ†çš„å…¬å¼å°±æ˜¯å¯¹ä¸€ä¸ªç‰¹å®šèŠ‚ç‚¹çš„è¯¯å·®å¯¹æŸä¸ªæƒé‡çš„å½±å“ã€‚ä½†æ˜¯ï¼Œè¿™åªè€ƒè™‘äº†è¿™ä¸ªç‰¹å®šèŠ‚ç‚¹ã€‚
***
ç„¶åæ˜¯è€å¸ˆè¿›è¡Œrewriteçš„å…¬å¼ï¼š
- æ”¹æˆäº†ä»¥ä¸‹å½¢å¼ï¼ˆåˆšåˆšé‚£ä¸ªå…¬å¼ï¼‰:
$$\sum_{j'=1}^{n^{l_0}} \left(X_{j'}^{l_0} - t_{j'}\right) \cdot \left(f_{j'}^{l_0}\right)' \left(S_{j'}^{l_0}\right) \cdot \frac{\partial S_{j'}^{l_0}}{\partial w_{j_0i_0}^{l_0}}$$
- $\sum_{j'=1}^{n^{l_0}}$ è¡¨ç¤ºç°åœ¨æ˜¯å¯¹è¿™ä¸€å±‚çš„æ¯ä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œè€ƒè™‘
- å¯¹äºæ¯ä¸ªèŠ‚ç‚¹æ¥è¯´ï¼Œè¯¯å·®å¯¹æƒé‡çš„ä¾èµ–å…³ç³»éœ€è¦æ‹†è§£å‡ºæ¥ã€‚å› ä¸ºè¯¯å·®æ¥è‡ªäºæœ€åçš„è¾“å‡ºï¼Œè€Œæ¯ä¸ªæƒé‡çš„å½±å“æ˜¯é€šè¿‡å±‚ä¸å±‚ä¹‹é—´çš„è¾“å…¥æ¥é—´æ¥å½±å“çš„ã€‚ä¸ºäº†æ›´æ¸…æ¥šåœ°æè¿°è¿™ç§å½±å“ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é“¾å¼æ³•åˆ™æ¥æŠŠæ¯ä¸€éƒ¨åˆ†çš„å˜åŒ–æ˜ç¡®åœ°è¡¨è¾¾å‡ºæ¥ã€‚
å°±æ˜¯è¿™ç©æ„$\frac{\partial S_{j'}^{l_0}}{\partial w_{j_0i_0}^{l_0}}$
èŠ‚ç‚¹çš„è¾“å…¥æ˜¯æƒé‡å’Œå‰ä¸€å±‚çš„è¾“å‡ºçš„ä¹˜ç§¯ï¼Œå› æ­¤å¯¹è¿™ä¸ªæƒé‡æ±‚åå¯¼æ—¶ï¼Œç»“æœå°±æ˜¯å‰ä¸€å±‚çš„è¾“å‡ºï¼ˆå› ä¸ºå…¶ä»–éƒ¨åˆ†æ˜¯å¸¸æ•°ï¼‰ã€‚
**ç®€è€Œè¨€ä¹‹ï¼Œç¬¬äºŒç§å°±æ˜¯åœ¨è€ƒè™‘æ•´å±‚æ‰€æœ‰èŠ‚ç‚¹çš„è¯¯å·®**
$$\frac{\partial E}{\partial w_{j_0i_0}^{l_0}} = \begin{cases} \sum_{j'=1}^{n^{l_0}} \left(X_{j'}^{l_0} - t_{j'}\right) \cdot \left(f_{j'}^{l_0}\right)' \left(S_{j'}^{l_0}\right) \cdot \frac{\partial S_{j'}^{l_0}}{\partial w_{j_0i_0}^{l_0}}, & \text{if } l = l_0, \\ \sum_{j=1}^{n^l}\left(X_j^l - t_j\right) \cdot \left(f_j^l\right)' \left(S_j^l\right) \cdot \sum_{i=1}^{n^{l-1}}w_{ji}^{l-1} \cdot\left( \dots \cdot \left(f_{j'}^{l_0}\right)' \left(S_{j'}^{l_0}\right) \cdot \frac{\partial S_{j'}^{l_0}}{\partial w_{j_0i_0}^{l_0}} \right), & \text{if }l \neq l_0.\end{cases}$$
- ç„¶åå¯ä»¥å½’å¹¶æˆä»¥ä¸‹å…¬å¼ï¼š
$$\frac{\partial E}{\partial w_{j_0i_0}^{l_0}} = \sum_{j=1}^{n^l} \left(X_j^l - t_j\right) \cdot \left(f_j^l\right)' \left(S_j^l\right) \cdot \sum_{i=1}^{n^{l-1}} w_{ji}^{l-1} \cdot \left( \dots \cdot \left(f_{j'}^{l_0}\right)' \left(S_{j'}^{l_0}\right) \cdot \frac{\partial S_{j'}^{l_0}}{\partial w_{j_0i_0}^{l_0}} \right)$$
***
Step2: Rewrite the formula in a matrix form
![b2b1df5650e5d94ef2cb3b5ac679e6e.png](https://s2.loli.net/2024/12/01/sVnUQlK9hqkIHxw.png)
The objective of this step is to derive the relation between the partial derivatives between two layers.
***
1. $$\nabla_{X^l} E \triangleq \left( \frac{\partial E}{\partial X_1^l} \cdots \frac{\partial E}{\partial X_{n^l}^l} \right) = \left( X_1^l - t_1 \cdots X_{n^l}^l - t_{n^l} \right)$$
This is the gradient of the ğ‘™-th layer.

- ç„¶åæˆ‘ä»¬æ¥æ›¿æ¢$\left(f_{j_0}^{l_0}\right)'(S_{j_0}^{l_0})$
2. $$\frac{d X^l}{d S^l} = \frac{d f^l(S^l)}{d S^l} \triangleq \mathcal{J}_{S^l} f^l = \begin{pmatrix} \frac{d f_1^l}{d S_1^l} & \cdots & \frac{d f_1^l}{d S_{n^l}^l} \\ \vdots & \ddots & \vdots \\ \frac{d f_{n^l}^l}{d S_1^l} & \cdots & \frac{d f_{n^l}^l}{d S_{n^l}^l} \end{pmatrix} = \begin{pmatrix} \left(f_1^l\right)' \left(S_1^l\right) & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \left(f_{n^l}^l\right)' \left(S_{n^l}^l\right) \end{pmatrix}$$
æ¯ä¸ªç¥ç»å…ƒçš„æ¿€æ´»å‡½æ•°å¯¼æ•°ä¼šå½¢æˆä¸€ä¸ªå¯¹è§’çŸ©é˜µ
***
![fcf915fdc1929d1ceda7808339ea39d.png](https://s2.loli.net/2024/12/01/aXtVCYEfHwsSFKu.png)
3. $$ \frac{dS^l}{dX^{l-1}} = \begin{pmatrix} \frac{\partial S_1^l}{\partial X_1^{l-1}} & \cdots & \frac{\partial S_1^l}{\partial X_{n^{l-1}}^{l-1}} \\ \vdots & \ddots & \vdots \\ \frac{\partial S_{n^l}^l}{\partial X_1^{l-1}} & \cdots & \frac{\partial S_{n^l}^l}{\partial X_{n^{l-1}}^{l-1}} \end{pmatrix} = \begin{pmatrix} w_{11}^l & \cdots & w_{1n^{l-1}}^l \\ \vdots & \ddots & \vdots \\ w_{n^l1}^l & \cdots & w_{n^ln^{l-1}}^l \end{pmatrix} \triangleq W^l. $$
4. $$ \frac{\partial S^{l_0}}{\partial w_{j_0i_0}^{l_0}} \triangleq \begin{pmatrix} \frac{\partial S_1^{l_0}}{\partial w_{j_0i_0}^{l_0}} & \cdots & \frac{\partial S_{n^{l_0}}^{l_0}}{\partial w_{j_0i_0}^{l_0}} \end{pmatrix}^T = \begin{pmatrix} 0 & \cdots & \frac{\partial S_{j_0}^{l_0}}{\partial w_{j_0i_0}^{l_0}} & \cdots & 0 \end{pmatrix}^T = \begin{pmatrix} 0 & \cdots & X_{i_0}^{l_0-1} & \cdots & 0 \end{pmatrix}^T, $$
where $ S^{l_0} = \begin{pmatrix} S_1^{l_0} & \cdots & S_{n^{l_0}}^{l_0} \end{pmatrix}^T. $
åœ¨è®¡ç®—åå¯¼æ•°æ—¶ï¼Œæˆ‘ä»¬åªå…³å¿ƒè¿™ä¸ªè¾“å…¥å¦‚ä½•å˜åŒ–ã€‚å› æ­¤ï¼Œå¯¹äºç‰¹å®šçš„æƒé‡$w_{j_0i_0}^{l_0}$,åªæœ‰ä¸è¿™ä¸ªæƒé‡ç›´æ¥ç›¸è¿çš„å‰ä¸€å±‚èŠ‚ç‚¹çš„è¾“å‡ºæ‰å¯¹è¾“å…¥äº§ç”Ÿå½±å“ï¼Œå…¶å®ƒä¸ç›¸å…³çš„èŠ‚ç‚¹å¯¹è¾“å…¥çš„åå¯¼æ•°æ˜¯ 0ã€‚
- æ ¹æ®ä¹‹å‰çš„1ï¼Œ2ï¼Œ3ï¼Œ4 ï¼Œ å…¬å¼èƒ½è¡¨è¾¾æˆä»¥ä¸‹å½¢å¼ï¼š
$$ \frac{\partial E}{\partial w_{j_0i_0}^{l_0}} = \frac{dE}{dX^l} \cdot \frac{dX^l}{dS^l} \cdot \frac{dS^l}{dX^{l-1}} \cdot \frac{dX^{l-1}}{dS^{l-1}} \cdots \frac{\partial S^{l_0}}{\partial w_{j_0i_0}^{l_0}} = \nabla_{X^l}E \cdot J_{S^l}f^l \cdot w^l \cdot J_{S^{l-1}}f^{l-1} \cdots \frac{\partial S^{l_0}}{\partial w_{j_0i_0}^{l_0}}. $$
- let $$\delta^h = \nabla_{X^l}E \cdot J_{S^l}f^l \cdot w^l \cdot J_{S^{l-1}}f^{l-1} \cdot \ldots \cdot J_{S^h}f^h$$
- Then we have:

$$\frac{\partial E}{\partial w_{j_0i_0}^{l_0}} = \delta^{l_0} \cdot \frac{\partial S^{l_0}}{\partial w_{j_0i_0}^{l_0}} = \delta^{l_0} \cdot \left(0 \, \cdots \, X_{i_0}^{l_0-1} \, \cdots \, 0\right)^T$$
æˆ‘ä»¬è¿˜èƒ½å¾—åˆ°ï¼š$$\delta^{h-1} = \delta^h \cdot w^h \cdot J_{S^{h-1}}f^{h-1}$$
è¿™æ ·æˆ‘ä»¬å°±èƒ½é€’å½’çš„è®¡ç®—ã€‚
### Comparing to the NaÃ¯ve Computation


- Computing** $\delta^{h-1}$ **in terms of** $\delta^h$ **avoids obvious duplicate computation, more specifically multiplication, of layers** $h$ and beyond. è®¡ç®— $\delta^{h-1}$ åŸºäº $\delta^h$ å¯ä»¥é¿å…æ˜¾è€Œæ˜“è§çš„é‡å¤è®¡ç®—ï¼Œç‰¹åˆ«æ˜¯å±‚ $h$ åŠå…¶ä¹‹åçš„ä¹˜æ³•æ“ä½œã€‚
- **Multiplying starting from the output layer â€“ propagating the error** *backwards* â€“ **means that each step simply multiplies a vector** $\delta^h$ **and a weight matrix** $w^{h-1}$, **which makes matrix operations so important in neural network forward/backpropagation.** ä»è¾“å‡ºå±‚å¼€å§‹ä¹˜æ³•è®¡ç®—â€”â€”å³è¯¯å·®çš„åå‘ä¼ æ’­â€”â€”æ„å‘³ç€æ¯ä¸€æ­¥åªéœ€å°†ä¸€ä¸ªå‘é‡ $\delta^h$ å’Œä¸€ä¸ªæƒé‡çŸ©é˜µ $w^{h-1}$ ç›¸ä¹˜ï¼Œè¿™ä½¿å¾—çŸ©é˜µæ“ä½œåœ¨ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸­å°¤ä¸ºé‡è¦ã€‚
### Learning Algorithm
![13fa547093114eb17b99ca87c9e7e05.png](https://s2.loli.net/2024/12/02/E5qOZt38kQK1AUW.png)





