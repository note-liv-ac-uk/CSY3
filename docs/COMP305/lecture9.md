# Lecture 8
<font size="4">Hongye Qian</font> 

### Topic 3 Hebbâ€™s Rules
#### 1. ANN Learning Rules
![845cc26dece61888d5f6e9e8aa41255.png](https://s2.loli.net/2024/10/26/Dn25vJqdhI1W4pA.png)

#### 2. Running example
![e508c5112e3a0127668bae844b4e6d5.png](https://s2.loli.net/2024/10/26/3CzILQokTANaBSe.png)
- Round 0:
  ![51e766199f437b8e2212e2d42046d1c.png](https://s2.loli.net/2024/10/26/D4GP8nLpmOwRbij.png)
  1. Beginning: $C=1,t=0, w_1^0=1, w_2^0=1, w_3^0=1, w_4^0=1$, we suppose $a_1^0=1, a_2^0=0, a_3^0=1, a_4^0=0$
$S^0 = \sum_{i=1}^{4} w_i^0 a_i^0 \\
= w_1^0 \times a_1^0 + w_2^0 \times a_2^0 + w_3^0 \times a_3^0 + w_4^0 \times a_4^0 \\
= 1 \times 1 + 1 \times 0 + 1 \times 1 + 1 \times 0 = 2 \geq \theta
$ so the $X^1=1$
  1.  ä»£å…¥ä¸‹é¢çš„å…¬å¼ï¼š
   $\Delta w_i^0 = C a_i^0 X^1 \\w_i^1 = w_i^0+ \Delta w_i^0$
  $\Delta w_1^0 = 1 \times 1 \times 1 = 1, \quad w_1^1 = w_1^0 + \Delta w_1^0 = 1 + 1 = 2; \\
\Delta w_2^0 = 1 \times 0 \times 1 = 0, \quad w_2^1 = w_2^0 + \Delta w_2^0 = 1 + 0 = 1; \\
\Delta w_3^0 = 1 \times 1 \times 1 = 1, \quad w_3^1 = w_3^0 + \Delta w_3^0 = 1 + 1 = 2; \\
\Delta w_4^0 = 1 \times 0 \times 1 = 0, \quad w_4^1 = w_4^0 + \Delta w_4^0 = 1 + 0 = 1;
$
æ³¨æ„ä¸Šé¢çš„æƒé‡å·²ç»å‘ç”Ÿå˜æ¢äº†ã€‚The weights have been changed.
- Round 1: $t=1$
  ![4328cf0c614efde7a0656a44101ac78.png](https://s2.loli.net/2024/10/26/AixqcGrCO7EayMe.png)
  1. $S^1 = \sum_{i=1}^{4} w_i^1 a_i^1 \\
= w_1^1 \times a_1^1 + w_2^1 \times a_2^1 + w_3^1 \times a_3^1 + w_4^1 \times a_4^1 \\
= 2 \times 1 + 1 \times 0 + 2 \times 1 + 1 \times 0 = 4 \geq \theta
$ so the $X^2=1$
  1. ä»£å…¥ä¸‹é¢çš„å…¬å¼:
  $\Delta w_i^1 = C a_i^1 X^2 \\
w_i^2 = w_i^1 + \Delta w_i^1
$
$\Delta w_1^1 = 1 \times 1 \times 1 = 1, \quad w_1^2 = w_1^1 + \Delta w_1^1 = 2 + 1 = 3; \\
\Delta w_2^1 = 1 \times 0 \times 1 = 0, \quad w_2^2 = w_2^1 + \Delta w_2^1 = 1 + 0 = 1; \\
\Delta w_3^1 = 1 \times 1 \times 1 = 1, \quad w_3^2 = w_3^1 + \Delta w_3^1 = 2 + 1 = 3; \\
\Delta w_4^1 = 1 \times 0 \times 1 = 0, \quad w_4^2 = w_4^1 + \Delta w_4^1 = 1 + 0 = 1;
$
æ³¨æ„æƒé‡çš„å˜åŒ–ã€‚ Please note the changes of weights.
- Round 2: $t=2$,è¿™æ—¶å€™æ³¨æ„ä¸€ä¸‹è¾“å…¥çš„å››ä¸ª$a_i^2$æ˜¯$1, 0, 1, 0$,è¿™ä¸ªè¾“å…¥æ—¶éšæœºçš„ï¼Œä¸æ˜¯å›ºå®šä¸å˜çš„ï¼Œpptä¸Šåœ¨t=2çš„æ—¶å€™çš„è¾“å…¥å°±æ˜¯1, 0, 1, 0
![94c5299932ded228fb3d3932581929c.png](https://s2.loli.net/2024/10/26/RkiCeMmXwaY5rAp.png)
  1. $S^2 = \sum_{i=1}^{4} w_i^2 a_i^2 \\
= w_1^2 \times a_1^2 + w_2^2 \times a_2^2 + w_3^2 \times a_3^2 + w_4^2 \times a_4^2 \\
= 3 \times 1 + 1 \times 1 + 2 \times 0 + 1 \times 0 = 4 \geq \theta$ so the $X^3=1$
  1. ä»£å…¥ä»¥ä¸‹å…¬å¼ï¼š
   $\Delta w_i^2 = C a_i^2 X^3 \\
w_i^3 = w_i^2 + \Delta w_i^2$
$\Delta w_1^2 = 1 \times 1 \times 1 = 1, \quad w_1^3 = w_1^2 + \Delta w_1^2 = 3 + 1 = 4; \\
\Delta w_2^2 = 1 \times 1 \times 1 = 1, \quad w_2^3 = w_2^2 + \Delta w_2^2 = 1 + 1 = 2; \\
\Delta w_3^2 = 1 \times 0 \times 1 = 0, \quad w_3^3 = w_3^2 + \Delta w_3^2 = 3 + 0 = 3; \\
\Delta w_4^2 = 1 \times 0 \times 1 = 0, \quad w_4^3 = w_4^2 + \Delta w_4^2 = 1 + 0 = 1;
$
- **æˆ‘è¿™é‡Œå†™çš„æœ‰ç‚¹å¿«äº†ï¼Œè¦æ˜¯çœ‹ä¸æ‡‚ä¸Šlecture9 P13å¼€å§‹çœ‹**

#### 3. Meaning behind Hebbâ€™s Rule
![d7131b0a5bc4e98c24bbff0fcb3e056.png](https://s2.loli.net/2024/10/26/OQnqVYIfpLolwi2.png)
-  **Intuition**: If two adjacent neurons always fire together, they should have strong relation (large weight)è¿™å…¶å®å°±æ˜¯ä¸€ä¸ªä¾‹å­è§£é‡Šäº†Lecture 8é‡Œçš„ç¬¬3ç‚¹çš„ç¬¬äºŒå°ç‚¹ï¼ˆç¬”è®°é‡Œï¼‰ã€‚

#### 4. Theory behind Hebbâ€™s Rule
- Assumption 1: 
  $X^{t+1} = g(S^t) = H(S^t - \theta) = 
\begin{cases} 
1, & S^t \geq \theta; \\
0, & S^t < \theta.
\end{cases}
\quad \Rightarrow \quad
X^{t+1} = g(S^t) = \sum_i w_i^t a_i^t$
è¿™ä¸ªä¹‹å‰çš„ç¬”è®°ä¸­æœ‰ï¼Œæ¿€æ´»å‡½æ•°è¢«å®šä¹‰ä¸ºè¶Šé˜¶å‡½æ•°(Heaviside å‡½æ•°)
- Assumption 2: All the inputs $a^t$ are from a given finite data set $D$ with the cardinality $N$. In each epoch, we train the neuron with all the inputs within the data set. We have infinite epochs. æ‰€æœ‰è¾“å…¥$a^t$ éƒ½æ¥è‡ªä¸€ä¸ªæœ‰é™æ•°æ®é›† $D$, å…¶åŸºæ•°ä¸º $N$ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ•°æ®é›† $D$ç”±$N$ä¸ªè¾“å…¥ç»„æˆã€‚åœ¨æ¯ä¸ªå‘¨æœŸä¸­ï¼Œç¥ç»å…ƒä¼šç”¨æ•°æ®é›†ä¸­æ‰€æœ‰çš„è¾“å…¥è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬æœ‰æ— é™çš„è®­ç»ƒå‘¨æœŸï¼Œè¿™æ„å‘³ç€ç¥ç»ç½‘ç»œä¸æ–­è¿›è¡Œå­¦ä¹ å’Œè°ƒæ•´ã€‚è¿™ä¹Ÿåæ˜ äº†Hebbçš„åè¨€:"Cells that fire together, wire together"
- discrete form to the continuous formï¼š
  discrete form : $\Delta w_i^t = C a_i^t X^{t+1}
$ æˆ‘ä»¬å°†$\Delta w_i^t$å–æé™ï¼Œæ—¶é—´è¶‹å‘äºæ— ç©·å°ï¼Œå·¦è¾¹å†™æˆ$\frac{dw_i}{dt}$, å³è¾¹ä»£å…¥æ¿€æ´»å‡½æ•°$X^{t+1} = g(S^t) = \sum_i w_i^t a_i^t$ï¼Œå› ä¸ºç°åœ¨tæ— é™å°ï¼Œå®ƒæ—¶ä¸€ç§è¿ç»­çš„æ¦‚å¿µ, $X = \sum_i w_i a_i
$, æ‰€ä»¥æˆ‘ä»¬å¾—åˆ° $\frac{dw}{dt} = C a \left( \sum_i w_i a_i \right)$
- ä¸ºäº†æ›´æ–¹ä¾¿åœ°è¿›è¡Œå‘é‡åŒ–æ“ä½œï¼Œå¯ä»¥å°†ä¸Šé¢çš„å…¬å¼è½¬åŒ–ä¸ºçŸ©é˜µå½¢å¼ã€‚å°†æ ‡é‡çš„åŠ æƒå’Œè¡¨ç¤ºä¸ºçŸ©é˜µä¹˜æ³•ï¼š
  è¾“å…¥å‘é‡ ğ‘ å¯ä»¥å†™æˆï¼š $[a_1, a_2, \ldots, a_n]^T$
  æƒé‡å‘é‡ ğ‘¤ å¯ä»¥å†™æˆï¼š$[w_1, w_2, \ldots, w_n]^T$
  æ‰€ä»¥æˆ‘ä»¬å¾—åˆ°: $\frac{dw}{dt} = C a a^T w$
- In each epoch, all the $N$ inputs within the data set are considered, thus we can use the average $\bar{a}$ over the data set as the increase rate, represented as $\bar{a} \bar{a}^T$ è¿™æ˜¯ä» Hebb æ³•åˆ™çš„è¿ç»­çŸ©é˜µå½¢å¼è¿›ä¸€æ­¥æ¨å¯¼å‡ºæ¥çš„ã€‚åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸä¸­ï¼Œä½¿ç”¨æ•°æ®é›†ä¸Šçš„å¹³å‡å€¼æ¥ä»£æ›¿å•ä¸ªè¾“å…¥çš„å¤–ç§¯ï¼Œä»è€Œå¾—åˆ°äº†è¿™ä¸ªæ›´æ–°çš„å½¢å¼ã€‚
æˆ‘ä»¬å¾—åˆ° $\frac{dw}{dt} = C \bar{a} \bar{a}^T w = \alpha w, \quad \text{with} \quad \alpha = C \bar{a} \bar{a}^T$
- Plus, The average $\bar{a}$ always exists, but we donâ€™t know what it is. The â€œ<u>average</u>â€ is the feature of the data set $D$  that is learnt by the Hebbâ€™s rule.
- æ­¤å¸¸å¾®æ–¹ç¨‹(ODE)çš„å”¯ä¸€è§£å¯ä»¥è¡¨ç¤ºä¸ºï¼š
  $W(t) = k_1 e^{\alpha_1 t} \delta_1 + k_2 e^{\alpha_2 t} \delta_2 + \cdots + k_m e^{\alpha_m t} \delta_m$
  å…¶ä¸­$k_i$æ˜¯åˆå§‹æ¡ä»¶å†³å®šçš„ç³»æ•°, $\delta_i$æ˜¯çŸ©é˜µ$\alpha$çš„ç¬¬$i$ä¸ªç‰¹å¾å‘é‡, $\alpha_i$æ˜¯çŸ©é˜µ$\alpha$çš„ç¬¬$i$ä¸ªç‰¹å¾å€¼
  When the sufficient time has passed, that is, ğ‘¡ is large enough, å¯ä»¥è¿‘ä¼¼ä¸º: $W(t) \approx k^* e^{\alpha^* t} \delta^*$
  å…¶ä¸­$\alpha^*$æ˜¯çŸ©é˜µçš„æœ€å¤§ç‰¹å¾å€¼ï¼Œ $\delta^*$æ˜¯ä¸æœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚
  è¿™è¡¨ç¤ºå½“ç»è¿‡è¶³å¤Ÿé•¿æ—¶é—´åï¼Œç³»ç»Ÿçš„è¡Œä¸ºå°†ä¸»è¦ç”±æœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„é¡¹æ¥ä¸»å¯¼ï¼Œå…¶ä»–é¡¹ç›¸å¯¹æ¥è¯´ä¼šå˜å¾—ä¸é‚£ä¹ˆé‡è¦ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºç”±æœ€å¤§ç‰¹å¾å€¼ä¸»å¯¼ï¼Œä¹Ÿå°±æ˜¯ç‰¹å¾å€¼çš„æŒ‡æ•°å¢é•¿å†³å®šäº†æ•´ä¸ªç³»ç»Ÿçš„æœ€ç»ˆæ¼”åŒ–ã€‚
  è¾“å‡ºçš„è¿‘ä¼¼å€¼ä¸ºï¼š$X(t) \approx k^* e^{\alpha^* t} \delta^* a$

- **PCA and Hebb**
  PCAï¼ˆPrincipal Component Analysisï¼Œä¸»æˆåˆ†åˆ†æï¼‰ï¼Œè¿™ä¸ Hebb æ³•åˆ™æœ‰å¾ˆå¼ºçš„å…³è”ã€‚é€šè¿‡æœ€å¤§ç‰¹å¾å€¼ä¸»å¯¼çš„æ¦‚å¿µå¯ä»¥çœ‹å‡ºï¼ŒHebb æ³•åˆ™å®é™…ä¸Šä¸å¯»æ‰¾æ•°æ®é›†ä¸­æœ€å¤§æ–¹å·®æ–¹å‘ï¼ˆå³ä¸»æˆåˆ†ï¼‰æ˜¯ç›¸å…³çš„ã€‚è¿™æ„å‘³ç€é€šè¿‡ Hebb å­¦ä¹ è§„åˆ™ï¼Œç½‘ç»œå¯ä»¥å­¦ä¹ åˆ°æ•°æ®çš„ä¸»è¦ç‰¹å¾ï¼Œè¿™å’Œ PCA çš„ç›®æ ‡æ˜¯ç±»ä¼¼çš„ã€‚
- ![d7131b0a5bc4e98c24bbff0fcb3e056.png](https://s2.loli.net/2024/10/26/OQnqVYIfpLolwi2.png)
  **All the weights increase monotonously**: è¿™æ„å‘³ç€æ‰€æœ‰çš„æƒé‡éƒ½ä¼šå•è°ƒå¢åŠ ï¼Œå³æ¯æ¬¡æ›´æ–°æ—¶æƒé‡çš„å€¼éƒ½ä¼šå¢åŠ ï¼Œä¸ä¼šå‡å°‘ã€‚è¿™æ˜¯ Hebb å­¦ä¹ è§„åˆ™çš„ç»“æœï¼Œå› ä¸ºæ¯å½“è¾“å…¥å’Œè¾“å‡ºåŒæ—¶æ¿€æ´»æ—¶ï¼Œæƒé‡éƒ½ä¼šå¢åŠ ã€‚
  **Finally, each weight will become large enough such that any activated input can fire the neuron alone** æœ€ç»ˆï¼Œæ¯ä¸ªæƒé‡éƒ½ä¼šå˜å¾—è¶³å¤Ÿå¤§ï¼Œä»¥è‡³äºå•ç‹¬ä¸€ä¸ªè¢«æ¿€æ´»çš„è¾“å…¥å°±å¯ä»¥è§¦å‘ç¥ç»å…ƒçš„è¾“å‡ºã€‚è¿™æ„å‘³ç€å½“ç³»ç»Ÿç»è¿‡è¶³å¤Ÿé•¿çš„æ—¶é—´è®­ç»ƒåï¼Œåªè¦æŸä¸ªè¾“å…¥è¢«æ¿€æ´»ï¼ˆå³è¾“å…¥ä¸º 1ï¼‰ï¼Œå°±è¶³ä»¥æ¿€æ´»ç¥ç»å…ƒï¼Œè¿™ä¹Ÿæ˜¯ Hebb å­¦ä¹ è§„åˆ™çš„ä¸€ç§æ•ˆæœâ€”â€”å®ƒè®©ç½‘ç»œå­¦ä¼šå¼ºåŒ–æ¿€æ´»è·¯å¾„ï¼Œä½¿å¾—æŸäº›è¾“å…¥å¯¹è¾“å‡ºçš„å½±å“å˜å¾—æ›´åŠ æ˜¾è‘—.













