# Lecture 10
<font size="4">Hongye Qian</font> 
### Topic 4 Normalized Hebbâ€™s Rule (Ojaâ€™s Rule)
#### 1. Hebb's rule vs Normalized Hebbâ€™s Rule
![082c334c5e41cb16ab540bd3befb2fa.png](https://s2.loli.net/2024/10/27/cfByhOLMTRxjWGA.png)
 - All the weights increase **monotonously**. Finally, each weight will become large enough such that any activated input can fire the neuron alone. 
![30dcf831841116c9c387b428c2cdd01.png](https://s2.loli.net/2024/10/27/Qka2D9Fh1N8UOWm.png)
- Intuition: Weights will NOT increase monotonously. The most frequent firing pattern will be remembered and finally dominate. å¿˜äº†normalizedæµç¨‹çš„çœ‹ä¸Šä¸€ä¸ªç¬”è®°ã€‚

#### 2. Why does Normalized Hebbâ€™s Rule Converge?
-  The exact dynamics of the Ojaâ€™s rule have been solved by Wyatt and Elfaldel 1995. Wyattå’ŒElfadelåœ¨1995å¹´è§£å†³äº†Ojaè§„åˆ™çš„å…·ä½“åŠ¨åŠ›å­¦é—®é¢˜ã€‚Ojaè§„åˆ™æ˜¯Hebbè§„åˆ™çš„ä¸€ç§æ”¹è¿›ï¼Œé€šè¿‡å¯¹æƒé‡çš„æ›´æ–°è¿‡ç¨‹è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶å…·æœ‰æ›´å¥½çš„æ”¶æ•›ç‰¹æ€§ã€‚
-  It shows that the weight $w \rightarrow \delta^*$, where $\delta^*$ is the eigenvector of the largest eigenvalue $\alpha^*$ of the matrix based on average $\bar{a}$ **æƒé‡çš„æ”¶æ•›**ï¼šä»–ä»¬å‘ç°ï¼Œä½¿ç”¨Ojaè§„åˆ™ï¼Œæƒé‡å‘é‡$w$ä¼šè¶‹å‘äº$\delta^*$, å…¶ä¸­$\delta^*$ æ˜¯åŸºäºçŸ©é˜µå¹³å‡å€¼ $\bar{a}$ çš„æœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ï¼ˆå¿˜äº†è¿™ä¸ªçš„å»ä¸Šä¸€ä¸ªç¬”è®°æ‰¾ï¼‰
- æ— å½’ä¸€åŒ–çš„é—®é¢˜ï¼šå¦‚æœæ²¡æœ‰å½’ä¸€åŒ–ï¼ˆå³ä½¿ç”¨åŸå§‹Hebbè§„åˆ™ï¼‰ï¼Œæ¯ä¸ªè¿æ¥çš„æƒé‡ä¼šä»¥ç‰¹å¾å€¼$\alpha_i$ æŒ‡æ•°çº§å¢é•¿ã€‚è¿™ä¼šå¯¼è‡´æƒé‡å¤±æ§ï¼Œæ— æ³•è¾¾åˆ°ç¨³å®šçš„çŠ¶æ€ã€‚
- å½’ä¸€åŒ–çš„ä¼˜åŠ¿ï¼šåœ¨ä½¿ç”¨å½’ä¸€åŒ–ï¼ˆå³Ojaè§„åˆ™ï¼‰çš„æƒ…å†µä¸‹ï¼Œåªæœ‰æœ€å¤§ç‰¹å¾å€¼$\alpha^*$ å¯¹åº”çš„ç‰¹å¾å‘é‡ä¼šæœ€ç»ˆâ€œç”Ÿå­˜â€ä¸‹æ¥ï¼Œå…¶ä½™çš„ç‰¹å¾å€¼å¯¹åº”çš„æƒé‡ä¼šé€æ¸å‡å¼±ã€‚è¿™æ ·å°±ä¿è¯äº†æƒé‡çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚
#### 3. Observations of the Running Example
- æ•°æ®é›†ï¼šç»™å®šçš„æ•°æ®é›†åªæœ‰ä¸€ä¸ªæ ·æœ¬ [1,0,1,0]ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä»…ä½¿ç”¨è¿™ä¸€ç»„æ•°æ®æ¥è®­ç»ƒç¥ç»å…ƒã€‚
- ç¥ç»å…ƒçš„æƒé‡ï¼šè®­ç»ƒåï¼Œç¥ç»å…ƒçš„æƒé‡å˜æˆ $[0.71,0.04,0.71,0.04]$ã€‚è¿™ç»„æƒé‡åæ˜ äº†è¾“å…¥ $[1,0,1,0]$ çš„ç‰¹å¾ã€‚è¿™æ„å‘³ç€ç¥ç»å…ƒå¯¹è¯¥è¾“å…¥æ ·æœ¬çš„ç‰¹æ€§è¿›è¡Œäº†â€œå­¦ä¹ â€ã€‚
- çŸ©é˜µè®¡ç®—ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å…¬å¼ $C \bar{a}^T \bar{a} = 1 \cdot \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \end{bmatrix} \cdot [1 \ 0 \ 1 \ 0] = \begin{bmatrix} 1 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 \\ 1 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}$, å¿˜äº†è¿™ä¸ªçš„å»çœ‹å‰é¢çš„ç¬”è®°ã€‚è¿™ä¸ªçŸ©é˜µåæ˜ äº†è¾“å…¥å‘é‡çš„ç‰¹å¾å…³ç³»ã€‚
- ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼šè¯¥çŸ©é˜µçš„æœ€å¤§ç‰¹å¾å€¼ä¸º2ï¼Œå¯¹åº”çš„ç‰¹å¾å‘é‡ä¸º $[0.7071,0,0.7071,0]$ã€‚ç‰¹å¾å‘é‡çš„2-èŒƒæ•°ä¸º1ï¼ˆå³ç‰¹å¾å‘é‡çš„é•¿åº¦ä¸º1ï¼‰ï¼Œè¡¨ç¤ºè¿™ä¸ªæ–¹å‘ä¸Šçš„ç‰¹å¾æœ€é‡è¦ã€‚
- å¿˜äº†çŸ©é˜µç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼æ€ä¹ˆæ±‚çš„ï¼Œå»çœ‹å¤§ä¸€çš„çº¿æ€§ä»£æ•°ï¼Œæˆ–è€…çœ‹ä¸‹é¢è¿™ä¸ªç½‘ç«™ <u>https://zhuanlan.zhihu.com/p/104980382 </u>
- æƒé‡ä¸å†å•è°ƒå¢åŠ ï¼Œè€Œæ˜¯æ ¹æ®è¾“å…¥çš„é‡è¦æ€§æ¥è°ƒæ•´ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¾“å…¥è¶Šé‡è¦ï¼Œ**å¯¹åº”çš„æƒé‡å°±è¶Šæ¥è¿‘1**ã€‚

#### 4. Hebbâ€™s Rules and Associative Learning
![0fa160d7752f4c92ddd9eab9af48351.png](https://s2.loli.net/2024/10/28/dLIkXxqfZzY3W42.png)
-  Applying Hebbâ€™s rules (including the original one and the normalized one) a neural network, we know that <u>the weight of connection increases between nodes having activated outputs simultaneously.</u> å½“æˆ‘ä»¬åœ¨ç¥ç»ç½‘ç»œä¸­åº”ç”¨Hebbè§„åˆ™æ—¶ï¼Œæ— è®ºæ˜¯åŸå§‹è§„åˆ™è¿˜æ˜¯å½’ä¸€åŒ–åçš„è§„åˆ™ï¼Œè¿æ¥æƒé‡ä¼šæ ¹æ®Hebbè§„åˆ™çš„å®šä¹‰è¿›è¡Œè°ƒæ•´ã€‚å½“ä¸¤ä¸ªèŠ‚ç‚¹ï¼ˆæˆ–ç¥ç»å…ƒï¼‰åœ¨åŒä¸€æ—¶é—´éƒ½è¢«æ¿€æ´»æ—¶ï¼Œå®ƒä»¬ä¹‹é—´çš„è¿æ¥ä¼šå˜å¾—æ›´å¼ºï¼Œä»è€Œå¢å¼ºå®ƒä»¬ä¹‹é—´çš„è”ç³»ã€‚
- è¿™ç§æƒé‡çš„è°ƒæ•´å’Œå¢å¼ºå¯ä»¥ç”¨äºè§£å†³å…³è”é—®é¢˜ï¼ˆassociation problemsï¼‰ï¼Œå³è¯†åˆ«æˆ–è®°ä½ä¸åŒè¾“å…¥ä¹‹é—´çš„å…³ç³»ã€‚ä¾‹å¦‚ï¼ŒæŸäº›ç‰¹å®šçš„è¾“å…¥æ¨¡å¼ä¼šå¯¼è‡´ç¥ç»å…ƒçš„æ¿€æ´»ï¼Œç¥ç»ç½‘ç»œå¯ä»¥â€œå­¦ä¹ â€åˆ°è¿™äº›å…³è”å¹¶è®°ä½å®ƒä»¬ã€‚
![f70bd8fcd1a0146728b1f7c30b31a4d.png](https://s2.loli.net/2024/10/28/l5Mo7XAL3qx6ZnU.png)
- Input vector $a = (a_1, a_2, \dots, a_n)$, æƒé‡å‘é‡ä¸º $W = (w_1, w_2, \dots, w_n)$
- Then the instant state of the output neuron is defined as dot product: $S = W \cdot a^T = \sum_{i=1}^n w_i a_i$ è¿™ä¸ªç‚¹ç§¯ï¼ˆæˆ–åŠ æƒå’Œï¼‰è¶Šå¤§ï¼Œè¡¨ç¤ºè¾“å…¥å’Œæƒé‡è¶Šâ€œç›¸ä¼¼â€ï¼Œå³ä¸¤è€…è¶Šâ€œæ¥è¿‘â€å½¼æ­¤ã€‚This weighted sum will be greater if the vectors ğ’‚ and ğ‘¾ are similar, i.e., close to each other.
![27765b7603cf88a257fa457f368f012.png](https://s2.loli.net/2024/10/28/yrEJvaG5Ddl3XOQ.png)
![8c77d33ae5341d79d0115adfa4b0e54.png](https://s2.loli.net/2024/10/28/48L2IJUybi1kAZR.png) 
- Training of a neuron with normalised Hebbâ€™s learning rule results in a vector of weights of connections similar to the training input vector. å¦‚æœè¾“å…¥å‘é‡ ğ‘ å’Œæƒé‡å‘é‡ ğ‘Š ç›¸ä¼¼ï¼ˆå³æ–¹å‘ç›¸åŒæˆ–æ¥è¿‘ï¼‰ï¼Œç‚¹ç§¯çš„å€¼ä¼šæ›´å¤§ï¼Œä»è€Œç¥ç»å…ƒçš„è¾“å‡ºä¼šæ›´é«˜ã€‚è¿™æ„å‘³ç€Hebbè§„åˆ™åœ¨æ›´æ–°æƒé‡æ—¶ï¼Œä¼šä½¿æƒé‡å‘é‡æœè¾“å…¥å‘é‡çš„æ–¹å‘è°ƒæ•´ï¼Œä½¿å…¶æ›´æ¥è¿‘è¾“å…¥æ¨¡å¼ã€‚
- The neuron â€œremembersâ€ the training pattern via increasing weight of corresponding connections. 
- å¦‚æœè®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œæ¥æ”¶åˆ°ä¸€ä¸ª**ç±»ä¼¼äºè®­ç»ƒç”¨çš„è¾“å…¥å‘é‡** $a$ çš„æ–°è¾“å…¥å‘é‡ $a'$, é‚£ä¹ˆè¿™ä¸ªæ–°è¾“å…¥å‘é‡ $a'$ ä¹Ÿåº”è¯¥ä¸ç½‘ç»œçš„æƒé‡å‘é‡$W$ ç›¸ä¼¼ï¼Œä»è€Œäº§ç”Ÿç›¸ä¼¼çš„ç¥ç»å…ƒè¾“å‡ºã€‚

- One may say that , the network â€œrecognisesâ€ the new input or â€œassociatesâ€ it with the training one.
#### 5.  Associative Learning
![ea74ff792ae61236540c0d8997f0fcf.png](https://s2.loli.net/2024/10/28/YfrSiI29ROK5pWB.png)
-  This weighted sum ğ‘† will be greater if the vectors $a$ and $w$ are similar. i.e. close to each other
-  Definiton: Association is the task of mapping patterns to patterns. å…³è”å­¦ä¹ æ˜¯å°†ä¸€ä¸ªæ¨¡å¼æ˜ å°„åˆ°å¦ä¸€ä¸ªæ¨¡å¼çš„ä»»åŠ¡ã€‚å®ƒæ¶‰åŠè®°å¿†è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å…³ç³»ï¼Œè€Œä¸æ˜¯å•çº¯çš„è®°å¿†è¾“å…¥æœ¬èº«ã€‚
-    An associative memory is to learn and remember the mapping between two unrelated pattens.  å…³è”è®°å¿†çš„ç›®çš„æ˜¯å­¦ä¹ å’Œè®°ä½ä¸¤ä¸ªä¸ç›¸å…³çš„æ¨¡å¼ä¹‹é—´çš„æ˜ å°„ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡å­¦ä¹ ï¼Œç¥ç»ç½‘ç»œèƒ½å¤Ÿå°†è¾“å…¥å’Œæƒé‡åŒ¹é…ï¼Œå½¢æˆå…³è”ã€‚
-    Example:  For instance, a person may learn a word and its meaning before (learn the mapping between word and meaning). When the person reads the word that is spelled incorrectly, she or he may know it has the same meaning with the correct word by association (remember the mapping). ä¸€ä¸ªäººå¯èƒ½å…ˆå­¦ä¼šä¸€ä¸ªå•è¯åŠå…¶å«ä¹‰ï¼ˆå³â€œå•è¯â€å’Œâ€œå«ä¹‰â€ä¹‹é—´çš„æ˜ å°„ï¼‰ï¼Œå½“ä»–é‡åˆ°æ‹¼å†™é”™è¯¯çš„å•è¯æ—¶ï¼Œä»ç„¶å¯ä»¥é€šè¿‡å…³è”è®°å¿†è¯†åˆ«å‡ºå®ƒçš„å«ä¹‰ã€‚è¿™è¡¨æ˜ä»–é€šè¿‡â€œå…³è”â€è®°ä½äº†æ­£ç¡®æ‹¼å†™å’Œå«ä¹‰çš„å…³ç³»ã€‚

#### 6. Unsupervised Learning
- We do not label any input in the data set during learning. The weight updating in each iteration only involves the input, the output and the current weight. åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæ•°æ®é›†ä¸­çš„ä»»ä½•è¾“å…¥éƒ½ä¸éœ€è¦æ ‡è®°ã€‚æƒé‡çš„æ›´æ–°ä»…ä¾èµ–äºå½“å‰çš„è¾“å…¥ã€è¾“å‡ºå’Œå½“å‰çš„æƒé‡å€¼ã€‚
-  We do not care what the output pattern means. We care which inputs are considered similar by the network. That is, unsupervised learning. è¿™æ„å‘³ç€æˆ‘ä»¬ä¸éœ€è¦æä¾›â€œæ­£ç¡®â€æˆ–â€œé”™è¯¯â€çš„æ ‡ç­¾ï¼Œç½‘ç»œä¼šè‡ªåŠ¨å­¦ä¹ è¾“å…¥çš„æ¨¡å¼ã€‚
-  Unsupervised learning is a type of machine learning in which the algorithm is not provided with any pre-assigned labels or scores for the training data. As a result, unsupervised learning algorithms must first self-discover any naturally occurring patterns in that training data set.
-   It will be much clearer when we extend the network with single output to the network with multiple outputs.
#### 7. From Single Output to Multiple outputs
![aa46cb2bf974ea7ff27c2bd28062a0e.png](https://s2.loli.net/2024/10/28/VJunw74iC5RZLNI.png)
- When there are multiple outputs, weight vector becomes weight matrix.
- **For a single output network**, we can strengthen the link between an input pattern and an output by normalized Hebbâ€™s rule. The network can therefore recognize other similar inputs;
- **For a multi-output network**, it is supposed to recognize multiple types of patterns. Each input could be recognized as either one type of pattern, or none of them. (Clustering)














